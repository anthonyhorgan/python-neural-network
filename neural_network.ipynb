{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Deep Learning Assignment 1**:\n",
        "Student name: Anthony Horgan  \n",
        "id: 17452572  \n",
        "All descriptions and code is taken from the lecture notes."
      ],
      "metadata": {
        "id": "tn7IovhKkEbS"
      },
      "id": "tn7IovhKkEbS"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "12afcc5e-787a-4af6-8bfa-7d3a70cd254a",
      "metadata": {
        "id": "12afcc5e-787a-4af6-8bfa-7d3a70cd254a"
      },
      "outputs": [],
      "source": [
        "# Package imports\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import sklearn.datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "import pickle\n",
        "\n",
        "# Display plots inline and change default figure size\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "circles_fname = \"/content/drive/MyDrive/Colab Notebooks/dl_assignment1/circles600.csv\"\n",
        "blobs_fname = \"/content/drive/MyDrive/Colab Notebooks/dl_assignment1/blobs300.csv\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZlfdc6E9Tsw",
        "outputId": "5fa903ab-5340-4d7f-eb5c-88e1e1ca90dd"
      },
      "id": "xZlfdc6E9Tsw",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05f53574-f981-4c61-9972-2491f30814bc",
      "metadata": {
        "id": "05f53574-f981-4c61-9972-2491f30814bc"
      },
      "source": [
        "**Part 1: Logistic Regression**  \n",
        "This is an implementation of logistic regression to create a binary classification model. Logistic regression is a simple classification algorithm.  \n",
        "Logistic regression consists of a learnable linear function followed by a non-linear activation function (sigmoid).  \n",
        "For each attribute in the input data (independent variables), there is a trainable weight parameter (which is just a number). We multiply each input value with its corresponding weight, sum them all together and add a bias (bias is another trainable parameter). This result then gets fed through the sigmoid function to produce a value in the range [0, 1]. During the learning phase, a random input from the training set is chosen, the output is calculated, the log loss is used to calculate the difference between the output of logistic regression, the weights and bias parameters are then updated using stochastic gradient descent. (The weight and bias values are updated according to the derivative of the loss function with respect the weight and bias values multiplied by a learning rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "5213ee3b-3e53-47db-a025-121709724650",
      "metadata": {
        "id": "5213ee3b-3e53-47db-a025-121709724650"
      },
      "outputs": [],
      "source": [
        "epsilon = 1e-8\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def log_loss(y_hat, y):\n",
        "    # add epsilon to avoid log(0)\n",
        "    return - np.mean(y * np.log(y_hat + epsilon) + (1 - y) * np.log(1 - y_hat + epsilon))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7dac439d-ed1e-4603-b666-ab1047c6c40c",
      "metadata": {
        "id": "7dac439d-ed1e-4603-b666-ab1047c6c40c"
      },
      "outputs": [],
      "source": [
        "# function to handle loading and sampling of data as well as storing and printing loss and accuracy \n",
        "def train(net, n_iter, train_set=None, val_set=None, evaluate_inter=1000, verbose=True):\n",
        "    '''\n",
        "    trains a classifier\n",
        "            Parameters:\n",
        "                    net: classifier to train. net must have forward(x) method and train_step(x, y) method\n",
        "                    train_set : tuple of (input_data, input_labels). Used to train net\n",
        "                    val_set: tuple of (input_data, input_labels). optional. Used to evaluate net.\n",
        "                    evaluate_inter: (int), loss and accuracy scores will be caulated on train_set and val_set every evaluate_inter iterations\n",
        "                    verbose: (bool) whether to print out loss and accuracy scores throughout training\n",
        "\n",
        "            Returns:\n",
        "                    train_loss_list, val_loss_list: lists of loss scores from every evaluation throughout training\n",
        "    '''\n",
        "    if verbose:\n",
        "        print(\"starting training\")\n",
        "        print(f\"iter\\ttrain loss\\ttrain acc\\tval loss\\tval acc\")\n",
        "    X_train, y_train = train_set\n",
        "    X_val = y_val = None\n",
        "    if val_set:\n",
        "        X_val, y_val = val_set\n",
        "    # lists which are used to store loss values from each evaluation step throughout training\n",
        "    train_loss_list = []\n",
        "    val_loss_list = []\n",
        "    n_samples = X_train.shape[0]\n",
        "    for iteration in range(n_iter):\n",
        "        # get random sample\n",
        "        sample_idx = np.random.choice(np.arange(n_samples))\n",
        "        # forward step\n",
        "        x_single, y_single = X_train[sample_idx], y_train[sample_idx]\n",
        "        # backward step\n",
        "        net.train_step(x_single, y_single)\n",
        "        \n",
        "        if iteration % evaluate_inter == 0:\n",
        "            # every set amount of iterations, compute loss and accuracy on train and val set\n",
        "            # get classifier output\n",
        "            out = net(X_train)\n",
        "            # get loss on entire set\n",
        "            train_loss = log_loss(out, y_train)\n",
        "            train_loss_list.append(train_loss)\n",
        "            # get accuracy on entire set\n",
        "            preds = out > 0.5\n",
        "            train_acc = np.sum(np.squeeze(preds) == y_train)/X_train.shape[0]\n",
        "            val_loss = None\n",
        "            val_acc = None\n",
        "            # if val set is given, compute loss and accuracy on val set\n",
        "            if val_set:\n",
        "                # get classifier output\n",
        "                out = net(X_val)\n",
        "                # get loss on entire set\n",
        "                val_loss = log_loss(out, y_val)\n",
        "                # get accuracy on entire set\n",
        "                preds = out > 0.5\n",
        "                val_acc = np.sum(np.squeeze(preds) == y_val)/X_val.shape[0]\n",
        "                val_loss_list.append(val_loss)\n",
        "                \n",
        "                # print loss and accuracy\n",
        "                if verbose:\n",
        "                    print(f\"{iteration:<6}\\t{train_loss:.4f}\\t\\t{train_acc:.4f}\\t\\t{val_loss:.4f}\\t\\t{val_acc:.4f}\")\n",
        "            elif verbose:\n",
        "                print(f\"{iteration:<6}\\t{train_loss:.4f}\\t\\t{train_acc:.4f}\")\n",
        "            \n",
        "    return train_loss_list, val_loss_list"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "483e3d8a-2a2f-426a-9f99-b9ca715bc625",
      "metadata": {
        "id": "483e3d8a-2a2f-426a-9f99-b9ca715bc625"
      },
      "source": [
        "Each of the classifiers has a forward method to handle forward propagation and a train_step function to handle backward propagation (because backprop is handled in network object, optimization parameters need to be passed to init).  \n",
        "Each of the classifiers can handle an arbitrary number of on the forward pass but not on the backward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "fc42a717-e2d7-4f2c-922b-921d9d60702e",
      "metadata": {
        "tags": [],
        "id": "fc42a717-e2d7-4f2c-922b-921d9d60702e"
      },
      "outputs": [],
      "source": [
        "class LRClassifier:\n",
        "    def __init__(self, n_attributes, lr=0.1):\n",
        "        # init weights and biases\n",
        "        self.w = np.random.randn(n_attributes)\n",
        "        self.b = np.random.randn()\n",
        "        self.lr = lr\n",
        "    \n",
        "    def forward(self, x):\n",
        "        z = np.matmul(x, self.w) + self.b\n",
        "        a = sigmoid(z)\n",
        "        return a\n",
        "    \n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def train_step(self, x_single, y_single):\n",
        "        # handlesgradient descent\n",
        "\n",
        "        # get classifier output\n",
        "        y_hat= self.forward(x_single)\n",
        "        \n",
        "        # get loss\n",
        "        loss = log_loss(y_hat, y_single)\n",
        "        \n",
        "        # calculate and store gradient for weights and bias\n",
        "        w_gradients = (y_hat - y_single) * x_single\n",
        "        b_gradient = y_hat - y_single\n",
        "    \n",
        "        # update weights and bias\n",
        "        self.w -= self.lr * w_gradients\n",
        "        self.b -= self.lr * b_gradient\n",
        "        \n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c55e64dc-49a1-4276-8693-13a7f569658b",
      "metadata": {
        "id": "c55e64dc-49a1-4276-8693-13a7f569658b"
      },
      "source": [
        "**Part 2**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def read_data(filename, val_size, test_size):\n",
        "    # this function was extended from sample code given in LoadDataset.ipynb\n",
        "    # Use pandas to read the CSV file as a dataframe\n",
        "    df = pd.read_csv(filename)\n",
        "\n",
        "    # The y values are those labelled 'Class': extract their values\n",
        "    y = df['Class'].values\n",
        "\n",
        "    # The x values are all other columns\n",
        "    del df['Class']   # drop the 'Class' column from the dataframe\n",
        "    X = df.values     # convert the remaining columns to a numpy array\n",
        "\n",
        "    # val_size = test_size = 0.15\n",
        "\n",
        "    n_train = int(X.shape[0] * (1 - val_size - test_size))\n",
        "    n_val = int(X.shape[0] * val_size)\n",
        "    n_test = int(X.shape[0] * test_size)\n",
        "\n",
        "    perm = np.random.permutation(X.shape[0])\n",
        "    X = X[perm]\n",
        "    y = y[perm]\n",
        "    X_train, X_val, X_test = X[:n_train], X[n_train:n_train+n_val], X[n_train+n_val:]\n",
        "    y_train, y_val, y_test = y[:n_train], y[n_train:n_train+n_val], y[n_train+n_val:]\n",
        "    \n",
        "    return (X_train, y_train), (X_val, y_val), (X_test, y_test)"
      ],
      "metadata": {
        "id": "Dgpqq9Xb_S8v"
      },
      "id": "Dgpqq9Xb_S8v",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train logistic regression on blobs and circles dataset\n",
        "for filename in [blobs_fname, circles_fname]:\n",
        "    print(filename)\n",
        "    eval_inter = 1000\n",
        "    train_set, val_set, test_set = read_data(filename, val_size=0.15, test_size=0.15)\n",
        "    n_samples, n_attributes = train_set[0].shape\n",
        "    net = LRClassifier(n_attributes, lr=0.1)\n",
        "    # train logistic regression for 100 epochs\n",
        "    train_loss_list, val_loss_list = train(net, 10_000, train_set=train_set, val_set=val_set, evaluate_inter=eval_inter)\n",
        "    X_test, y_test = test_set\n",
        "    out = net(X_test)\n",
        "    preds = out > 0.5\n",
        "    test_acc = np.sum(np.squeeze(preds) == y_test)/X_test.shape[0]\n",
        "    print(f\"test accuracy: {test_acc:.4f}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAfBRyrG-P8y",
        "outputId": "b4e65dd4-99d7-4b41-b3bc-09fb2a9022fb"
      },
      "id": "wAfBRyrG-P8y",
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/dl_assignment1/blobs300.csv\n",
            "starting training\n",
            "iter\ttrain loss\ttrain acc\tval loss\tval acc\n",
            "0     \t0.3043\t\t0.8667\t\t0.2724\t\t0.8667\n",
            "1000  \t0.0393\t\t0.9905\t\t0.0085\t\t1.0000\n",
            "2000  \t0.0247\t\t0.9905\t\t0.0065\t\t1.0000\n",
            "3000  \t0.0207\t\t0.9905\t\t0.0035\t\t1.0000\n",
            "4000  \t0.0181\t\t0.9905\t\t0.0038\t\t1.0000\n",
            "5000  \t0.0189\t\t1.0000\t\t0.0062\t\t1.0000\n",
            "6000  \t0.0146\t\t0.9952\t\t0.0021\t\t1.0000\n",
            "7000  \t0.0138\t\t1.0000\t\t0.0024\t\t1.0000\n",
            "8000  \t0.0126\t\t0.9952\t\t0.0014\t\t1.0000\n",
            "9000  \t0.0118\t\t1.0000\t\t0.0014\t\t1.0000\n",
            "test accuracy: 1.0000\n",
            "\n",
            "/content/drive/MyDrive/Colab Notebooks/dl_assignment1/circles600.csv\n",
            "starting training\n",
            "iter\ttrain loss\ttrain acc\tval loss\tval acc\n",
            "0     \t0.7955\t\t0.5286\t\t0.8527\t\t0.4444\n",
            "1000  \t0.7032\t\t0.6857\t\t0.7444\t\t0.6111\n",
            "2000  \t0.6999\t\t0.3619\t\t0.6877\t\t0.4444\n",
            "3000  \t0.7003\t\t0.6238\t\t0.7300\t\t0.5556\n",
            "4000  \t0.6994\t\t0.5286\t\t0.7327\t\t0.4444\n",
            "5000  \t0.6958\t\t0.6762\t\t0.6990\t\t0.6556\n",
            "6000  \t0.6928\t\t0.6833\t\t0.6988\t\t0.6000\n",
            "7000  \t0.6941\t\t0.6881\t\t0.7101\t\t0.6111\n",
            "8000  \t0.6986\t\t0.5286\t\t0.7338\t\t0.4444\n",
            "9000  \t0.6984\t\t0.5762\t\t0.7203\t\t0.4889\n",
            "test accuracy: 0.5778\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic regression can easily solve the blobs dataset. It gets almost 100% train and val accuracy after 1000 iterations. This is because the blobs dataset is linearly separable.  \n",
        "Logistic regression struggles with the circles dataset. After 10,000 iterations it does no better than 50% which is as good as random guessing. It does get slightly better than 50% but not much. The circles dataset is not linearly separable."
      ],
      "metadata": {
        "id": "rSjIZ0iPAypQ"
      },
      "id": "rSjIZ0iPAypQ"
    },
    {
      "cell_type": "markdown",
      "id": "13bc8839-3fea-4541-830c-9cd7aae9b1f5",
      "metadata": {
        "id": "13bc8839-3fea-4541-830c-9cd7aae9b1f5"
      },
      "source": [
        "**Part 3**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A shallow neural network can be seen as an extension of logistic regression.  \n",
        "Whereas logistic regrssion consisted only of a single output node, a shallow neural network consists of a single layer of (multiple) hidden nodes and an output node. Each node has weights and a bias. For the first layer, each node multiplies its weight by the corresponding input attributes, sums this together, adds the bias and feeds it through a sigmoid activation function to produce the output for that node. The final node multiplies its weights by the corresponding outputs of the nodes in the previous layer, sums this together, adds the bias and sends this through a sigmoid activation function to produce the network output.  \n",
        "The training process is the same as for logistic regression except the gradients for the weights/biases in the hidden layer need to be calculated differently to the weights/biases in the output layer."
      ],
      "metadata": {
        "id": "kw6SYUCCEV0Q"
      },
      "id": "kw6SYUCCEV0Q"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "567d77d9-0fee-4a63-b7f7-82336a007e76",
      "metadata": {
        "id": "567d77d9-0fee-4a63-b7f7-82336a007e76"
      },
      "outputs": [],
      "source": [
        "def sigmoid_inverse(x):\n",
        "    # inverse of sigmoid function, useful when we need to calculate z from activation during the backward pass\n",
        "    return -np.log((1 / x) - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f6df0042-0210-42a6-8aa3-54cf4f0af2a2",
      "metadata": {
        "tags": [],
        "id": "f6df0042-0210-42a6-8aa3-54cf4f0af2a2"
      },
      "outputs": [],
      "source": [
        "class ShallowNN:\n",
        "    def __init__(self, n_attributes, n_hidden_nodes, lr=0.1):\n",
        "        self.lr = lr\n",
        "        \n",
        "        # initialize weights\n",
        "        self.w_out = np.random.randn(1, n_hidden_nodes)\n",
        "        self.b_out = np.random.randn(1, 1)\n",
        "        self.w_hidden = np.random.randn(n_hidden_nodes, n_attributes)\n",
        "        self.b_hidden = np.random.randn(n_hidden_nodes, 1)\n",
        "        # a_hidden_cache stores the activations from the hidden layer because they are needed during backprop\n",
        "        self.a_hidden_cache = None    \n",
        "    \n",
        "    def forward(self, x):\n",
        "        # hidden layer\n",
        "        x = x.transpose()\n",
        "        z_hidden = np.matmul(self.w_hidden, x) + self.b_hidden\n",
        "        a_hidden = sigmoid(z_hidden)\n",
        "        # cache activation for later use in backprop\n",
        "        self.a_hidden_cache = a_hidden\n",
        "\n",
        "        # output layer\n",
        "        z_out = np.matmul(self.w_out, a_hidden) + self.b_out\n",
        "        a_out = sigmoid(z_out)\n",
        "        return a_out\n",
        "    \n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "    \n",
        "    def train_step(self, x_single, y_single):\n",
        "        y_hat= self.forward(x_single.reshape(1, -1))\n",
        "        # get loss\n",
        "        loss = log_loss(y_hat, y_single)\n",
        "        \n",
        "        # -----gradients for output layer--------------\n",
        "        w_out_gradients = (y_hat - y_single) * self.a_hidden_cache.transpose()\n",
        "        b_out_gradient = y_hat - y_single\n",
        "        \n",
        "        self.w_out -= self.lr * w_out_gradients\n",
        "        self.b_out -= self.lr * b_out_gradient\n",
        "        \n",
        "        # -----gradients for hidden layer--------------\n",
        "        z = sigmoid_inverse(self.a_hidden_cache)\n",
        "        # dz[l] =         f'(z[l])                    *  sum (       dz[l+1]      *    W[l+1] )\n",
        "        # dz = (np.exp(-z) / np.square(1 + np.exp(-z))) * np.sum((y_hat - y_single) * self.w_out, axis=0, keepdims=True).transpose()\n",
        "        dz = (np.exp(-z) / np.square(1 + np.exp(-z))) * (y_hat - y_single) * self.w_out.transpose()\n",
        "        w_hidden_gradients = np.outer(dz, x_single)\n",
        "        b_hidden_gradient = dz\n",
        "        \n",
        "        self.w_hidden -= self.lr * w_hidden_gradients\n",
        "        self.b_hidden -= self.lr * b_hidden_gradient\n",
        "        \n",
        "        \n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test shallow network on blobs and circles\n",
        "filename = blobs_fname \n",
        "\n",
        "print(\"blobs\")\n",
        "train_set, val_set, test_set = read_data(filename, val_size=0.15, test_size=0.15)\n",
        "n_samples, n_attributes = train_set[0].shape\n",
        "net = ShallowNN(n_attributes, n_hidden_nodes=5, lr=0.1)\n",
        "train_loss_list, val_loss_list = train(net, 10_000, train_set=train_set, val_set=val_set, evaluate_inter=1000)\n",
        "\n",
        "X_test, y_test = test_set\n",
        "out = net(X_test)\n",
        "preds = out > 0.5\n",
        "test_acc = np.sum(np.squeeze(preds) == y_test)/X_test.shape[0]\n",
        "print(f\"test accuracy: {test_acc:.4f}\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EijioBk2_qwy",
        "outputId": "a990387b-f97a-4def-a18a-19df8e446396"
      },
      "id": "EijioBk2_qwy",
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "blobs\n",
            "starting training\n",
            "iter\ttrain loss\ttrain acc\tval loss\tval acc\n",
            "0     \t1.1287\t\t0.4762\t\t0.9968\t\t0.5556\n",
            "1000  \t0.0476\t\t0.9952\t\t0.0575\t\t0.9778\n",
            "2000  \t0.0261\t\t0.9952\t\t0.0435\t\t0.9778\n",
            "3000  \t0.0208\t\t1.0000\t\t0.0281\t\t0.9778\n",
            "4000  \t0.0159\t\t0.9952\t\t0.0357\t\t0.9778\n",
            "5000  \t0.0130\t\t0.9952\t\t0.0281\t\t0.9778\n",
            "6000  \t0.0112\t\t0.9952\t\t0.0237\t\t0.9778\n",
            "7000  \t0.0102\t\t0.9952\t\t0.0232\t\t0.9778\n",
            "8000  \t0.0089\t\t0.9952\t\t0.0197\t\t0.9778\n",
            "9000  \t0.0077\t\t0.9952\t\t0.0152\t\t1.0000\n",
            "test accuracy: 1.0000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shallow network works just as well as logistic regression. This result is expected"
      ],
      "metadata": {
        "id": "caOeInavGWOe"
      },
      "id": "caOeInavGWOe"
    },
    {
      "cell_type": "code",
      "source": [
        "filename = circles_fname\n",
        "train_set, val_set, test_set = read_data(filename, val_size=0.15, test_size=0.15)\n",
        "n_samples, n_attributes = train_set[0].shape\n",
        "for n_hidden_nodes in range(1, 5):\n",
        "    net = ShallowNN(n_attributes, n_hidden_nodes=n_hidden_nodes, lr=0.1)\n",
        "    print(f\"n hidden nodes = {n_hidden_nodes}\")\n",
        "    train_loss_list, val_loss_list = train(net, 10_000, train_set=train_set, val_set=val_set, evaluate_inter=9000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HipWxQhMGl9F",
        "outputId": "9a12a417-3fa9-403f-aa30-a65ee16b3b90"
      },
      "id": "HipWxQhMGl9F",
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n hidden nodes = 1\n",
            "starting training\n",
            "iter\ttrain loss\ttrain acc\tval loss\tval acc\n",
            "0     \t0.8574\t\t0.4976\t\t0.7952\t\t0.5556\n",
            "9000  \t0.6443\t\t0.6905\t\t0.6638\t\t0.6889\n",
            "18000 \t0.5762\t\t0.6905\t\t0.6168\t\t0.6889\n",
            "n hidden nodes = 2\n",
            "starting training\n",
            "iter\ttrain loss\ttrain acc\tval loss\tval acc\n",
            "0     \t0.8946\t\t0.5024\t\t0.9713\t\t0.4444\n",
            "9000  \t0.3881\t\t0.8690\t\t0.4144\t\t0.8556\n",
            "18000 \t0.4124\t\t0.8476\t\t0.4531\t\t0.8333\n",
            "n hidden nodes = 3\n",
            "starting training\n",
            "iter\ttrain loss\ttrain acc\tval loss\tval acc\n",
            "0     \t0.6865\t\t0.5024\t\t0.7006\t\t0.4444\n",
            "9000  \t0.0568\t\t0.9976\t\t0.0598\t\t1.0000\n",
            "18000 \t0.0218\t\t0.9976\t\t0.0213\t\t1.0000\n",
            "n hidden nodes = 4\n",
            "starting training\n",
            "iter\ttrain loss\ttrain acc\tval loss\tval acc\n",
            "0     \t0.7576\t\t0.4976\t\t0.7078\t\t0.5556\n",
            "9000  \t0.0600\t\t1.0000\t\t0.0645\t\t1.0000\n",
            "18000 \t0.0259\t\t0.9976\t\t0.0337\t\t0.9889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shallow network can easily solve the circles dataset with a few hidden nodes. From tuning the number of hidden nodes, it seems that 3 nodes is the minimum to achieve 100% accuracy."
      ],
      "metadata": {
        "id": "ZnwOjm0jJOla"
      },
      "id": "ZnwOjm0jJOla"
    },
    {
      "cell_type": "code",
      "source": [
        "filename = circles_fname\n",
        "print(\"blobs\")\n",
        "train_set, val_set, test_set = read_data(filename, val_size=0.15, test_size=0.15)\n",
        "n_samples, n_attributes = train_set[0].shape\n",
        "net = ShallowNN(n_attributes, n_hidden_nodes=3, lr=0.1)\n",
        "train_loss_list, val_loss_list = train(net, 10_000, train_set=train_set, val_set=val_set, evaluate_inter=1000, verbose=False)\n",
        "\n",
        "X_test, y_test = test_set\n",
        "out = net(X_test)\n",
        "preds = out > 0.5\n",
        "test_acc = np.sum(np.squeeze(preds) == y_test)/X_test.shape[0]\n",
        "print(f\"test_acc = {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYE5Fw5gJGwJ",
        "outputId": "964a6d24-1841-4e93-9b20-9502cc082ab0"
      },
      "id": "aYE5Fw5gJGwJ",
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "blobs\n",
            "test_acc = 0.9889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 4**"
      ],
      "metadata": {
        "id": "P45GCDl5KxMw"
      },
      "id": "P45GCDl5KxMw"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "927f270f-8a0e-4d2f-96aa-5d6ee6276188",
      "metadata": {
        "tags": [],
        "id": "927f270f-8a0e-4d2f-96aa-5d6ee6276188"
      },
      "outputs": [],
      "source": [
        "# load image data\n",
        "def load_cifar(train_val=False, test=False, val_proportion=0.1):\n",
        "    # loads cifar data (only bird and airplane classes)\n",
        "    # if train_val=True, combine data from all 5 batch files\n",
        "    # if test=True return data from test batch\n",
        "    assert(train_val != test)\n",
        "    if train_val:\n",
        "        filepaths = [\"/content/drive/MyDrive/Colab Notebooks/dl_assignment1/cifar-10-batches-py/data_batch_1\",\n",
        "                     \"/content/drive/MyDrive/Colab Notebooks/dl_assignment1/cifar-10-batches-py/data_batch_2\",\n",
        "                     \"/content/drive/MyDrive/Colab Notebooks/dl_assignment1/cifar-10-batches-py/data_batch_3\",\n",
        "                     \"/content/drive/MyDrive/Colab Notebooks/dl_assignment1/cifar-10-batches-py/data_batch_4\",\n",
        "                     \"/content/drive/MyDrive/Colab Notebooks/dl_assignment1/cifar-10-batches-py/data_batch_5\",\n",
        "                     ]\n",
        "    elif test:\n",
        "        filepaths = [\"/content/drive/MyDrive/Colab Notebooks/dl_assignment1/cifar-10-batches-py/test_batch\"]\n",
        "    else:\n",
        "        raise Exception\n",
        "    # accumulated data/labels \n",
        "    acc_data = np.empty(0)\n",
        "    acc_labels = np.empty(0)\n",
        "    for filepath in filepaths:\n",
        "        with open(filepath, \"rb\") as f:\n",
        "            # load data from pickle file\n",
        "            images_dict = pickle.load(f, encoding=\"bytes\")\n",
        "            all_data = images_dict[b'data']\n",
        "            all_labels = images_dict[b'labels']\n",
        "            \n",
        "        # get idxs of images and labels for bird and plane classes only\n",
        "        idxs = []\n",
        "        for i, label in enumerate(all_labels):\n",
        "            if label == 0 or label == 2:\n",
        "                idxs.append(i)\n",
        "        \n",
        "        if len(acc_data) <= 0:\n",
        "            acc_data = all_data[idxs]\n",
        "            acc_labels = np.array(all_labels)[idxs]\n",
        "        else:\n",
        "            acc_data = np.concatenate((acc_data, all_data[idxs]), axis=0)\n",
        "            acc_labels = np.concatenate((acc_labels, np.array(all_labels)[idxs]), axis=0)\n",
        "\n",
        "\n",
        "    # convert labels to 0 and 1\n",
        "    acc_labels = (acc_labels > 0).astype(np.int32)\n",
        "    # convert rgb images to grayscale (average over 3 colour channels)\n",
        "    acc_data = acc_data.reshape(acc_data.shape[0], 3, 32, 32)\n",
        "    acc_data = np.mean(acc_data, axis=1)\n",
        "    acc_data = acc_data.reshape(acc_data.shape[0], -1)\n",
        "    # normalize the data to range [0, 1]\n",
        "    acc_data /= 255\n",
        "    perm = np.random.permutation(acc_data.shape[0])\n",
        "    if train_val:\n",
        "        # randomly shuffle data if train_val=True\n",
        "        acc_data = acc_data[perm]\n",
        "        acc_labels = acc_labels[perm]\n",
        "\n",
        "        # split data into train and val sets\n",
        "        n_val = int(acc_data.shape[0] * val_proportion)\n",
        "        train_set = (acc_data[n_val:], acc_labels[n_val:])\n",
        "        val_set = (acc_data[:n_val], acc_labels[:n_val])\n",
        "        return train_set, val_set\n",
        "    else:\n",
        "        return acc_data, acc_labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, val_set = load_cifar(train_val=True, val_proportion=0.1)\n",
        "test_set = load_cifar(test=True)\n",
        "for n_hidden_nodes in [10, 128, 512]:\n",
        "    print(f\"n hidden nodes: {n_hidden_nodes}\")\n",
        "    n_samples, n_attributes = train_set[0].shape\n",
        "    net = ShallowNN(n_attributes, n_hidden_nodes=n_hidden_nodes, lr=0.1)\n",
        "    train_loss_list, val_loss_list = train(net, 100_000, train_set=train_set, val_set=val_set, evaluate_inter=10_000, verbose=True)\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70Q7zUO_PER4",
        "outputId": "2b1a3840-c872-4f8a-b14c-dbb0b3726f48"
      },
      "id": "70Q7zUO_PER4",
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n hidden nodes: 10\n",
            "starting training\n",
            "iter\ttrain loss\ttrain acc\tval loss\tval acc\n",
            "0     \t0.7107\t\t0.5359\t\t0.7198\t\t0.5190\n",
            "10000 \t0.6677\t\t0.6027\t\t0.6750\t\t0.5980\n",
            "20000 \t0.6554\t\t0.6133\t\t0.6576\t\t0.6080\n",
            "30000 \t0.6545\t\t0.5780\t\t0.6712\t\t0.5610\n",
            "40000 \t0.6851\t\t0.5663\t\t0.6943\t\t0.5490\n",
            "50000 \t0.7758\t\t0.5142\t\t0.7793\t\t0.5180\n",
            "60000 \t0.7163\t\t0.5888\t\t0.7226\t\t0.5850\n",
            "70000 \t0.7534\t\t0.5918\t\t0.7599\t\t0.5800\n",
            "80000 \t0.6444\t\t0.6420\t\t0.6612\t\t0.6310\n",
            "90000 \t0.6227\t\t0.6540\t\t0.6413\t\t0.6420\n",
            "\n",
            "n hidden nodes: 128\n",
            "starting training\n",
            "iter\ttrain loss\ttrain acc\tval loss\tval acc\n",
            "0     \t3.8229\t\t0.5003\t\t3.7808\t\t0.5010\n",
            "10000 \t0.7383\t\t0.5883\t\t0.7647\t\t0.5760\n",
            "20000 \t1.7654\t\t0.5041\t\t1.8092\t\t0.5020\n",
            "30000 \t1.0483\t\t0.5096\t\t1.0547\t\t0.5100\n",
            "40000 \t1.7501\t\t0.5012\t\t1.7611\t\t0.5010\n",
            "50000 \t1.5551\t\t0.5114\t\t1.5187\t\t0.5040\n",
            "60000 \t1.2620\t\t0.5257\t\t1.2929\t\t0.5180\n",
            "70000 \t1.0877\t\t0.5161\t\t1.1048\t\t0.5090\n",
            "80000 \t1.2249\t\t0.5689\t\t1.2575\t\t0.5580\n",
            "90000 \t1.3369\t\t0.5198\t\t1.3865\t\t0.5120\n",
            "\n",
            "n hidden nodes: 512\n",
            "starting training\n",
            "iter\ttrain loss\ttrain acc\tval loss\tval acc\n",
            "0     \t8.8056\t\t0.5000\t\t8.8598\t\t0.4970\n",
            "10000 \t8.3790\t\t0.5021\t\t8.3837\t\t0.5010\n",
            "20000 \t8.1968\t\t0.5038\t\t8.2372\t\t0.5030\n",
            "30000 \t3.1732\t\t0.5187\t\t3.2475\t\t0.5090\n",
            "40000 \t6.5200\t\t0.5033\t\t6.5282\t\t0.5030\n",
            "50000 \t6.6663\t\t0.5092\t\t6.7658\t\t0.5070\n",
            "60000 \t3.7610\t\t0.5259\t\t3.8647\t\t0.5200\n",
            "70000 \t5.8031\t\t0.5060\t\t5.8595\t\t0.5050\n",
            "80000 \t5.3053\t\t0.5056\t\t5.3364\t\t0.5050\n",
            "90000 \t4.3571\t\t0.5080\t\t4.4183\t\t0.5020\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From previous testing 0.1 seems like a good learning rate for SGD on this problem.  \n",
        "The shallow network does not perform well on the cifar data. Interestingly, from our limited hyperparameter tuning, the accuracy of the model seems to generally get lower as the number of hidden nodes increases. This may be that the wider networks need longer to train. In any case, these results are not statistically significant."
      ],
      "metadata": {
        "id": "5pqWfF6mawRS"
      },
      "id": "5pqWfF6mawRS"
    },
    {
      "cell_type": "code",
      "source": [
        "# retrain 10 node network and test on test set\n",
        "\n",
        "train_set, val_set = load_cifar(train_val=True, val_proportion=0.1)\n",
        "test_set = load_cifar(test=True)\n",
        "n_samples, n_attributes = train_set[0].shape\n",
        "net = ShallowNN(n_attributes, n_hidden_nodes=10, lr=0.1)\n",
        "train_loss_list, val_loss_list = train(net, 100_000, train_set=train_set, val_set=val_set, evaluate_inter=10_000, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpb0z6iMbQ8M",
        "outputId": "bd077b4d-8f08-4d0d-e006-be2cb88e3017"
      },
      "id": "fpb0z6iMbQ8M",
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting training\n",
            "iter\ttrain loss\ttrain acc\tval loss\tval acc\n",
            "0     \t0.8743\t\t0.5019\t\t0.9004\t\t0.4810\n",
            "10000 \t0.6665\t\t0.5690\t\t0.6670\t\t0.5750\n",
            "20000 \t0.7859\t\t0.5891\t\t0.7796\t\t0.5910\n",
            "30000 \t0.6960\t\t0.5931\t\t0.6948\t\t0.5960\n",
            "40000 \t0.6992\t\t0.5081\t\t0.7147\t\t0.4830\n",
            "50000 \t0.7189\t\t0.5251\t\t0.7095\t\t0.5430\n",
            "60000 \t0.7206\t\t0.5572\t\t0.7149\t\t0.5650\n",
            "70000 \t0.7593\t\t0.5322\t\t0.7741\t\t0.5140\n",
            "80000 \t0.6843\t\t0.4986\t\t0.6847\t\t0.5210\n",
            "90000 \t0.6760\t\t0.5898\t\t0.6703\t\t0.5890\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qIi_cUGngqV4"
      },
      "id": "qIi_cUGngqV4",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, y_test = test_set\n",
        "out = net(X_test)\n",
        "preds = out > 0.5\n",
        "test_acc = np.sum(np.squeeze(preds) == y_test)/X_test.shape[0]\n",
        "print(f\"test_acc = {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBdNFDjVba6J",
        "outputId": "96525a99-7ea8-48f8-f165-a50ed5a63659"
      },
      "id": "DBdNFDjVba6J",
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_acc = 0.5970\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shallow net gets nearly 60% on the train set, which is quite good considering that it only has 10 hidden nodes."
      ],
      "metadata": {
        "id": "9OFMRo6BXxz6"
      },
      "id": "9OFMRo6BXxz6"
    },
    {
      "cell_type": "markdown",
      "id": "af13c64b-89e7-472f-89cf-e787620f92fb",
      "metadata": {
        "id": "af13c64b-89e7-472f-89cf-e787620f92fb"
      },
      "source": [
        "**Part 5**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The extensions I implemented are  \n",
        "    1: arbitrary number of layers  \n",
        "    2: Adam optimization   \n",
        "\n",
        "A Deep neural network is like a shallow neural network but with more hidden layers. The first hidden layer takes the independent variables as input (attributes of training example). The second hidden layer takes the outputs from the first hidden layer as input and so on until the output layer.  \n",
        "  \n",
        "Adam is an optimization algorithm which combines the concepts from Momentum and RMSProp. We store an exponential moving of the weight gradients (V) and an exponential moving average of the square of the weight gradients (S). Both V and S are initialized to 0. V can be thought of as the momentum of gradients and S can be thought of as the accelleration. For each iteration, the weight gradients are calculated as usual. Then V and S are updated according to hyperparameters beta1 and beta2 as can be seen in the code for DeepNN. Then VC and SC are calculated as can be seen in the code. VC and SC are bias corrected versions of V and S respectively. In SGD we update the weights according to the learning rate and the gradients. In Adam we update the weights according to the learning rate, VC and SC.\n",
        "  \n",
        "I also used relu activation function for all hidden layers."
      ],
      "metadata": {
        "id": "gtMU9NuoPL_O"
      },
      "id": "gtMU9NuoPL_O"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "2025a085-a854-4463-a58f-68cd3f818a38",
      "metadata": {
        "id": "2025a085-a854-4463-a58f-68cd3f818a38"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    return (x > 0) * x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "4b0d32c8-7334-46ce-b296-81c88477f5c9",
      "metadata": {
        "tags": [],
        "id": "4b0d32c8-7334-46ce-b296-81c88477f5c9"
      },
      "outputs": [],
      "source": [
        "epsilon = 1e-8\n",
        "\n",
        "class DeepNN:\n",
        "    def __init__(self, n_attributes, hidden_layer_sizes = (100,), lr=0.001, beta1=0.9, beta2=0.999):\n",
        "        # store optimization parameters\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.iter_counter = 0\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        self.a_cache = []\n",
        "        self.num_layers = len(hidden_layer_sizes) + 1\n",
        "        layer_sizes = [n_attributes] + hidden_layer_sizes\n",
        "        self.layer_sizes = layer_sizes\n",
        "        # initialize weights for each hidden layer\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            # store the weights and biases for each layer in a list\n",
        "            n_prev_nodes = layer_sizes[i]\n",
        "            n_curr_nodes = layer_sizes[i + 1]\n",
        "            # random weight initialization. Values are scaled down according to the size of the matrix\n",
        "            self.weights.append(np.random.randn(n_curr_nodes, n_prev_nodes) / np.sqrt(n_curr_nodes + n_prev_nodes))\n",
        "            self.biases.append(np.random.randn(n_curr_nodes, 1))\n",
        "        \n",
        "        # weight initialization for output layer\n",
        "        self.weights.append(np.random.randn(1, layer_sizes[-1]) / np.sqrt(layer_sizes[-1]))\n",
        "        self.biases.append(np.random.randn(1, 1))\n",
        "        \n",
        "        # initialize V and S values for Adam optimization\n",
        "        self.w_v = [None for _ in range(self.num_layers)]\n",
        "        self.b_v = [None for _ in range(self.num_layers)]\n",
        "        self.w_s = [None for _ in range(self.num_layers)]\n",
        "        self.b_s = [None for _ in range(self.num_layers)]\n",
        "        for l in range(self.num_layers):\n",
        "            self.w_v[l] = np.zeros_like(self.weights[l])\n",
        "            self.b_v[l] = np.zeros_like(self.biases[l])\n",
        "            self.w_s[l] = np.zeros_like(self.weights[l])\n",
        "            self.b_s[l] = np.zeros_like(self.biases[l])\n",
        "    \n",
        "    def forward(self, x):\n",
        "        a = x.transpose()\n",
        "        self.a_cache = []\n",
        "        for l, (w, b) in enumerate(zip(self.weights[:-1], self.biases[:-1])):\n",
        "            z = np.matmul(w, a) + b\n",
        "            a = relu(z)\n",
        "            self.a_cache.append(a)\n",
        "        \n",
        "        z = np.matmul(self.weights[-1], a) + self.biases[-1]\n",
        "        a = sigmoid(z)\n",
        "        self.a_cache.append(a)\n",
        "\n",
        "        self.a_cache.append(x.transpose())\n",
        "        return a.transpose().squeeze()\n",
        "    \n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def train_step(self, x_single, y_single):\n",
        "        y_hat = self(x_single.reshape(1, -1))\n",
        "        # get loss\n",
        "        loss = log_loss(y_hat, y_single)\n",
        "        \n",
        "        z_gradients = [None for _ in range(self.num_layers)]\n",
        "        w_gradients = [None for _ in range(self.num_layers)]\n",
        "        b_gradients = [None for _ in range(self.num_layers)]\n",
        "        \n",
        "        # calculate gradient for output layer\n",
        "        L = self.num_layers - 1\n",
        "        prev_a = self.a_cache[L - 1]\n",
        "        # {}\n",
        "        z_grad = (y_hat - y_single)\n",
        "        w_grad = z_grad * prev_a.transpose()\n",
        "        b_grad = z_grad\n",
        "        \n",
        "        z_gradients[L] = z_grad\n",
        "        w_gradients[L] = w_grad\n",
        "        b_gradients[L] = b_grad\n",
        "        \n",
        "        \n",
        "        # calculate gradients for all other hidden layers\n",
        "        for l in reversed(range(self.num_layers - 1)):\n",
        "            \n",
        "            prev_a = self.a_cache[l - 1]\n",
        "            curr_a = self.a_cache[l]\n",
        "            next_w = self.weights[l + 1]\n",
        "            \n",
        "            # -----gradients for hidden layer--------------\n",
        "            z_grad = (curr_a > 0) * np.sum(z_gradients[l + 1] * next_w, axis=0, keepdims=True).transpose()\n",
        "            w_grad = np.outer(z_grad, prev_a)\n",
        "            b_grad = z_grad\n",
        "            \n",
        "            z_gradients[l] = z_grad\n",
        "            w_gradients[l] = w_grad\n",
        "            b_gradients[l] = b_grad\n",
        "        \n",
        "        \n",
        "        # update V and S\n",
        "        for l in range(self.num_layers):\n",
        "            self.w_v[l] = (1 - self.beta1) * w_gradients[l] + self.beta1 * self.w_v[l]\n",
        "            self.b_v[l] = (1 - self.beta1) * b_gradients[l] + self.beta1 * self.b_v[l]\n",
        "            \n",
        "            self.w_s[l] = (1 - self.beta2) * np.square(w_gradients[l]) + self.beta2 * self.w_s[l]\n",
        "            self.b_s[l] = (1 - self.beta2) * np.square(b_gradients[l]) + self.beta2 * self.b_s[l]\n",
        "            \n",
        "        \n",
        "        # update weights\n",
        "        for l in range(self.num_layers):\n",
        "            w_v_c = self.w_v[l] / (1 - np.power(self.beta1, self.iter_counter + 1))\n",
        "            b_v_c = self.b_v[l] / (1 - np.power(self.beta1, self.iter_counter + 1))\n",
        "            \n",
        "            w_s_c = self.w_s[l] / (1 - np.power(self.beta2, self.iter_counter + 1))\n",
        "            b_s_c = self.b_s[l] / (1 - np.power(self.beta2, self.iter_counter + 1))\n",
        "            \n",
        "            self.weights[l] = self.weights[l] - self.lr * (w_v_c / (np.sqrt(w_s_c) + epsilon))\n",
        "            self.biases[l] = self.biases[l] - self.lr * (b_v_c / (np.sqrt(b_s_c) + epsilon))\n",
        "            \n",
        "            self.iter_counter += 1\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: In the next 2 cells I rewrite code for DNN as well as the train function using cupy functions so that I could train on a GPU."
      ],
      "metadata": {
        "id": "aNglFI5kIBvg"
      },
      "id": "aNglFI5kIBvg"
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "tags": [],
        "id": "-eghPO7yInNf"
      },
      "outputs": [],
      "source": [
        "epsilon = 1e-8\n",
        "\n",
        "class CPDeepNN:\n",
        "    def __init__(self, n_attributes, hidden_layer_sizes = [100], lr=0.001, beta1=0.9, beta2=0.999):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.iter_counter = 0\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        self.a_cache = []\n",
        "        self.num_layers = len(hidden_layer_sizes) + 1\n",
        "        layer_sizes = [n_attributes] + hidden_layer_sizes\n",
        "        # TODO watch out for this\n",
        "        self.layer_sizes = layer_sizes\n",
        "        # initialize weights for each hidden layer\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            n_prev_nodes = layer_sizes[i]\n",
        "            n_curr_nodes = layer_sizes[i + 1]\n",
        "            # random weight initialization. Values are scaled down according to the size of the matrix\n",
        "            self.weights.append(cp.random.randn(n_curr_nodes, n_prev_nodes) / cp.sqrt(n_curr_nodes + n_prev_nodes))\n",
        "            self.biases.append(cp.random.randn(n_curr_nodes, 1))\n",
        "        \n",
        "        # weight initialization for output layer\n",
        "        self.weights.append(cp.random.randn(1, layer_sizes[-1]) / cp.sqrt(layer_sizes[-1]))\n",
        "        self.biases.append(cp.random.randn(1, 1))\n",
        "        \n",
        "        # initialize V and S values for Adam optimization\n",
        "        self.w_v = [None for _ in range(self.num_layers)]\n",
        "        self.b_v = [None for _ in range(self.num_layers)]\n",
        "        self.w_s = [None for _ in range(self.num_layers)]\n",
        "        self.b_s = [None for _ in range(self.num_layers)]\n",
        "        for l in range(self.num_layers):\n",
        "            self.w_v[l] = cp.zeros_like(self.weights[l])\n",
        "            self.b_v[l] = cp.zeros_like(self.biases[l])\n",
        "            self.w_s[l] = cp.zeros_like(self.weights[l])\n",
        "            self.b_s[l] = cp.zeros_like(self.biases[l])\n",
        "    \n",
        "    def forward(self, x):\n",
        "        a = x.transpose()\n",
        "        self.a_cache = []\n",
        "        for l, (w, b) in enumerate(zip(self.weights[:-1], self.biases[:-1])):\n",
        "            z = cp.matmul(w, a) + b\n",
        "            a = relu(z)\n",
        "            self.a_cache.append(a)\n",
        "        \n",
        "        z = cp.matmul(self.weights[-1], a) + self.biases[-1]\n",
        "        a = sigmoid(z)\n",
        "        self.a_cache.append(a)\n",
        "\n",
        "        # TODO maybe get rid of these copy\n",
        "        # TODO here the inputs are appended  to a cache so that indexing by layer=-1 will retrieve the inputs\n",
        "        self.a_cache.append(x.transpose())\n",
        "        return a.transpose().squeeze()\n",
        "    \n",
        "    def __call__(self, x):\n",
        "        return self.forward(x)\n",
        "\n",
        "    def train_step(self, x_single, y_single):\n",
        "\n",
        "        y_hat = self(x_single.reshape(1, -1))\n",
        "        # get loss\n",
        "        loss = log_loss(y_hat, y_single)\n",
        "        \n",
        "        z_gradients = [None for _ in range(self.num_layers)]\n",
        "        w_gradients = [None for _ in range(self.num_layers)]\n",
        "        b_gradients = [None for _ in range(self.num_layers)]\n",
        "        \n",
        "        # calculate gradient for output layer\n",
        "        L = self.num_layers - 1\n",
        "        prev_a = self.a_cache[L - 1]\n",
        "        # {}\n",
        "        z_grad = (y_hat - y_single)\n",
        "        w_grad = z_grad * prev_a.transpose()\n",
        "        b_grad = z_grad\n",
        "        \n",
        "        z_gradients[L] = z_grad\n",
        "        w_gradients[L] = w_grad\n",
        "        b_gradients[L] = b_grad\n",
        "        \n",
        "        \n",
        "        # calculate gradients for all other hidden layers\n",
        "        for l in reversed(range(self.num_layers - 1)):\n",
        "            \n",
        "            prev_a = self.a_cache[l - 1]\n",
        "            curr_a = self.a_cache[l]\n",
        "            next_w = self.weights[l + 1]\n",
        "            \n",
        "            # -----gradients for hidden layer--------------\n",
        "            z_grad = (curr_a > 0) * cp.sum(z_gradients[l + 1] * next_w, axis=0, keepdims=True).transpose()\n",
        "            w_grad = cp.outer(z_grad, prev_a)\n",
        "            b_grad = z_grad\n",
        "            \n",
        "            z_gradients[l] = z_grad\n",
        "            w_gradients[l] = w_grad\n",
        "            b_gradients[l] = b_grad\n",
        "        \n",
        "        \n",
        "        # update V and S\n",
        "        for l in range(self.num_layers):\n",
        "            self.w_v[l] = (1 - self.beta1) * w_gradients[l] + self.beta1 * self.w_v[l]\n",
        "            self.b_v[l] = (1 - self.beta1) * b_gradients[l] + self.beta1 * self.b_v[l]\n",
        "            \n",
        "            self.w_s[l] = (1 - self.beta2) * cp.square(w_gradients[l]) + self.beta2 * self.w_s[l]\n",
        "            self.b_s[l] = (1 - self.beta2) * cp.square(b_gradients[l]) + self.beta2 * self.b_s[l]\n",
        "            \n",
        "        \n",
        "        # update weights\n",
        "        for l in range(self.num_layers):\n",
        "            w_v_c = self.w_v[l] / (1 - cp.power(self.beta1, self.iter_counter + 1))\n",
        "            b_v_c = self.b_v[l] / (1 - cp.power(self.beta1, self.iter_counter + 1))\n",
        "            \n",
        "            w_s_c = self.w_s[l] / (1 - cp.power(self.beta2, self.iter_counter + 1))\n",
        "            b_s_c = self.b_s[l] / (1 - cp.power(self.beta2, self.iter_counter + 1))\n",
        "            \n",
        "            self.weights[l] = self.weights[l] - self.lr * (w_v_c / (cp.sqrt(w_s_c) + epsilon))\n",
        "            self.biases[l] = self.biases[l] - self.lr * (b_v_c / (cp.sqrt(b_s_c) + epsilon))\n",
        "            \n",
        "            self.iter_counter += 1\n",
        "        "
      ],
      "id": "-eghPO7yInNf"
    },
    {
      "cell_type": "code",
      "source": [
        "# function to handle training loop\n",
        "def train_cp(net, n_iter, train_set=None, val_set=None, evaluate_inter=1000, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"starting training\")\n",
        "        print(f\"iter\\ttrain loss\\ttrain acc\\tval loss\\tval acc\")\n",
        "    # train logistic regression with SGD\n",
        "    X_train, y_train = train_set\n",
        "    X_train, y_train = cp.asarray(X_train), cp.asarray(y_train)\n",
        "    X_val = y_val = None\n",
        "    if val_set:\n",
        "        X_val, y_val = val_set\n",
        "    X_val, y_val = cp.asarray(X_val), cp.asarray(y_val)\n",
        "    train_loss_list = []\n",
        "    val_loss_list = []\n",
        "    n_samples = X_train.shape[0]\n",
        "    for iteration in range(n_iter):\n",
        "        # get random sample\n",
        "        sample_idx = np.random.choice(np.arange(n_samples))\n",
        "        # forward step\n",
        "        x_single, y_single = X_train[sample_idx], y_train[sample_idx]\n",
        "        # backward step\n",
        "        net.train_step(x_single, y_single)\n",
        "        \n",
        "        if iteration % evaluate_inter == 0:\n",
        "            # every set amount of iterations, compute loss and accuracy on train and val set\n",
        "            out = net(X_train)\n",
        "            train_loss = log_loss(out, y_train)\n",
        "            train_loss_list.append(train_loss.item())\n",
        "            preds = out > 0.5\n",
        "            train_acc = cp.sum(cp.squeeze(preds) == y_train)/X_train.shape[0]\n",
        "            val_loss = None\n",
        "            val_acc = None\n",
        "            # if val set is given, compute loss and accuracy on val set\n",
        "            if val_set:\n",
        "                # X_val, y_val = cp.asarray(X_val), cp.asarray(y_val)\n",
        "                out = net(X_val)\n",
        "                val_loss = log_loss(out, y_val)\n",
        "                preds = out > 0.5\n",
        "                val_acc = cp.sum(cp.squeeze(preds) == y_val)/X_val.shape[0]\n",
        "                val_loss_list.append(val_loss.item())\n",
        "                \n",
        "                if verbose:\n",
        "                    print(f\"{iteration:<6}\\t{train_loss:.4f}\\t\\t{train_acc:.4f}\\t\\t{val_loss:.4f}\\t\\t{val_acc:.4f}\")\n",
        "            elif verbose:\n",
        "                print(f\"{iteration:<6}\\t{train_loss:.4f}\\t\\t{train_acc:.4f}\")\n",
        "            \n",
        "    return train_loss_list, val_loss_list"
      ],
      "metadata": {
        "id": "QYvvqb43KeFN"
      },
      "id": "QYvvqb43KeFN",
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From previous hyperparameter tuning I found that 0.0001 was a good learning rate for Adam.  "
      ],
      "metadata": {
        "id": "xhTotrAfcsQi"
      },
      "id": "xhTotrAfcsQi"
    },
    {
      "cell_type": "code",
      "source": [
        "net = CPDeepNN(n_attributes=train_set[0].shape[1], hidden_layer_sizes=[512, 256, 128], lr=0.0001)\n",
        "train_loss_list, val_loss_list = train_cp(net, n_iter=100_000, train_set=train_set, val_set=val_set, evaluate_inter=10_000, verbose=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CG_jle4zJCNI",
        "outputId": "f34daed3-282d-493b-854c-0a09aa7d49b0"
      },
      "id": "CG_jle4zJCNI",
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting training\n",
            "iter\ttrain loss\ttrain acc\tval loss\tval acc\n",
            "0     \t0.7740\t\t0.5023\t\t0.7940\t\t0.4790\n",
            "10000 \t0.6107\t\t0.6789\t\t0.6041\t\t0.6930\n",
            "20000 \t0.6106\t\t0.6602\t\t0.5951\t\t0.6790\n",
            "30000 \t0.5431\t\t0.7357\t\t0.5364\t\t0.7530\n",
            "40000 \t0.5474\t\t0.7188\t\t0.5376\t\t0.7280\n",
            "50000 \t0.5139\t\t0.7510\t\t0.5215\t\t0.7480\n",
            "60000 \t0.5248\t\t0.7349\t\t0.5371\t\t0.7290\n",
            "70000 \t0.4973\t\t0.7662\t\t0.5045\t\t0.7610\n",
            "80000 \t0.5381\t\t0.7290\t\t0.5613\t\t0.7180\n",
            "90000 \t0.4770\t\t0.7760\t\t0.4894\t\t0.7700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "horiz_x_values = [i for i in range(len(train_loss_list))]\n",
        "plt.plot(horiz_x_values, train_loss_list, label=\"train\")\n",
        "plt.plot(horiz_x_values, val_loss_list, label=\"val\")\n",
        "plt.title(\"Loss Plot\")\n",
        "plt.xlabel(\"iteration in 1000s\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "cukv2QZidHPg",
        "outputId": "0cd81558-f806-46a4-8cbd-47f573d87474"
      },
      "id": "cukv2QZidHPg",
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7fd73b63b690>"
            ]
          },
          "metadata": {},
          "execution_count": 242
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, y_test = test_set\n",
        "out = net(X_test)\n",
        "preds = out > 0.5\n",
        "test_acc = np.sum(np.squeeze(preds) == y_test)/X_test.shape[0]\n",
        "print(f\"test_acc = {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM740k1Vh2Rd",
        "outputId": "aea111a9-3c61-499f-dc35-b388c9cf6332"
      },
      "id": "YM740k1Vh2Rd",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_acc = 0.7485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I think that 74% accuracy on the test set is quite an impressive result for a fully connected network. From the loss plot we can see that although the loss has plateaued somewhat, the network has not really overfitted and the loss is still trending downwards slightly so it could benefit from further training.  \n",
        "  \n",
        "From previous experiments (which I was not able to reproduce and print in this notebook due to google colab taking away my GPU), I found that making the network deeper and deeper did not seem to improve performance too much. I think the main imrprovement from part 3 to part 5 is the Adam optimization algorithm."
      ],
      "metadata": {
        "id": "wHoR752Virzm"
      },
      "id": "wHoR752Virzm"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NQO4S4KfjkwG"
      },
      "id": "NQO4S4KfjkwG",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "assignment1.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}